# -*- coding: utf-8 -*-
"""mlmed_practical2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lmp8mQRyh-dIRV3rYwZ0qdjkI9P5oRrr

#**1. Prepare the Data**
"""

from google.colab import drive
drive.mount('/content/drive')

training_zip_file = '/content/drive/My Drive/1327317/train.zip'
test_zip_file = '/content/drive/My Drive/1327317/test.zip'
dataset_base_path = '/content/drive/My Drive/1327317/'

# Unzip the dataset
!unzip -q "$training_zip_file" -d "$dataset_base_path"
!unzip -q "$test_zip_file" -d "$dataset_base_path"

"""#**2. Preprocess Data**"""

import pandas as pd
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

# Load and preprocess images
data_info = pd.read_csv(os.path.join(dataset_base_path, 'training_set_pixel_size_and_HC.csv'))
all_images = []
circumferences = []

for index, record in data_info.iterrows():
    img_path = os.path.join(dataset_base_path, 'training_set', record['filename'])
    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    if image is not None:
        resized_image = cv2.resize(image, (800, 540))
        all_images.append(resized_image)
        circumferences.append(record['head circumference (mm)'])

if not all_images:
    raise Exception("No images found. Verify paths.")

all_images = np.array(all_images, dtype='float32').reshape(-1, 800, 540, 1) / 255.0
circumferences = np.array(circumferences, dtype='float32')

# Split dataset
X_train, X_val, y_train, y_val = train_test_split(all_images, circumferences, test_size=0.2, random_state=42)

print(f"Successfully loaded and split {len(all_images)} images.")

"""#**3. Model**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Model
model = Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(800, 540, 1)),
    MaxPooling2D(2, 2),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(1)
])

model.compile(optimizer=Adam(0.001), loss='mean_squared_error', metrics=['mae'])

# Train
train_hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)

"""#**4. Evaluation**"""

# Evaluation
val_mae = model.evaluate(X_val, y_val)[1]
print(f"Validation MAE: {val_mae}")

import matplotlib.pyplot as plt

# Visualization
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.plot(train_hist.history['loss'], label='Train Loss')
plt.plot(train_hist.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(122)
plt.plot(train_hist.history['mae'], label='Train MAE')
plt.plot(train_hist.history['val_mae'], label='Val MAE')
plt.title('MAE over Epochs')
plt.legend()

plt.show()

# Prediction vs. Ground Truth Visualization**

predictions = model.predict(X_val).flatten()

# Visualize
plt.figure(figsize=(10, 6))
plt.plot(predictions[:25], 'x', label='Predicted')
plt.plot(y_val[:25], 'o', label='Actual')
plt.title('Prediction vs Actual')
plt.legend()
plt.show()

print(f"Number of predictions: {len(predictions)}")

# Error Analysis
errors = predictions - y_val
plt.hist(errors, bins=25, edgecolor='black')
plt.title('Prediction Error Distribution')
plt.xlabel('Error')
plt.ylabel('Count')
plt.show()

print(f"Mean Absolute Error: {np.mean(np.abs(errors)):.2f} mm")